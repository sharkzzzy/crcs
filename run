import torch
import torch.nn.functional as F
from diffusers import StableDiffusionXLPipeline
from PIL import Image

# ==============================================================================
# 1. åŸºç¡€è®¾ç½® (FP32 å…¨ç²¾åº¦æ¨¡å¼)
# ==============================================================================
device = "cuda" if torch.cuda.is_available() else "cpu"
model_id = "stabilityai/stable-diffusion-xl-base-1.0"

print("ğŸš€ Loading SDXL in FP32 (Safe Mode)...")
# æ³¨æ„ï¼šè¿™é‡Œå»æ‰äº† torch_dtype=torch.float16ï¼Œé»˜è®¤å°±æ˜¯ float32
pipe = StableDiffusionXLPipeline.from_pretrained(
    model_id,
    use_safetensors=True,
    variant="fp16", # è¿™é‡Œå¯ä»¥ç”¨ fp16 æƒé‡çš„å˜ä½“ï¼Œä½†åŠ è½½åå®ƒæ˜¯ float32
    # local_files_only=True # å¦‚æœä½ éœ€è¦å¼ºåˆ¶ç¦»çº¿ï¼Œå–æ¶ˆæ³¨é‡Šè¿™ä¸€è¡Œ
)
pipe.to(device)

# å†æ¬¡ç¡®ä¿ VAE ä¹Ÿæ˜¯ float32 (è™½ç„¶ä¸Šé¢æ²¡æŒ‡å®š fp16 åº”è¯¥é»˜è®¤å°±æ˜¯ï¼Œä½†ä¹°ä¸ªä¿é™©)
pipe.vae.to(dtype=torch.float32)
pipe.text_encoder.to(dtype=torch.float32)
pipe.text_encoder_2.to(dtype=torch.float32)
pipe.unet.to(dtype=torch.float32)

# ==============================================================================
# 2. è¾…åŠ©å‡½æ•°
# ==============================================================================
def get_masks(height, width, device):
    """ç”Ÿæˆå·¦å³åˆ†å±çš„ Maskï¼Œè¿”å›å½¢çŠ¶ [1, 1, h, w]"""
    latent_h, latent_w = height // 8, width // 8
    
    mask_left = torch.zeros(1, 1, latent_h, latent_w, device=device, dtype=torch.float32)
    mask_left[..., :latent_w // 2] = 1.0
    
    mask_right = torch.zeros(1, 1, latent_h, latent_w, device=device, dtype=torch.float32)
    mask_right[..., latent_w // 2:] = 1.0
    
    return mask_left, mask_right

# ==============================================================================
# 3. æ ¸å¿ƒé€»è¾‘
# ==============================================================================
@torch.no_grad()
def run_rccs(prompt_L, prompt_R, seed=42):
    height, width = 1024, 1024
    steps = 30 # ç¨å¾®å‡å°‘æ­¥æ•°ï¼ŒFP32 æ¯”è¾ƒæ…¢
    guidance_scale = 7.5
    
    print(f"Generating with Seed: {seed}")
    
    # 1. è·å– Embeddings
    # è¾…åŠ©å‡½æ•°ï¼šåŒæ—¶è·å– prompt_embeds å’Œ pooled_embeds
    def get_comp_embeddings(prompt):
        (pe, ne, pp, np) = pipe.encode_prompt(
            prompt, 
            device=device, 
            num_images_per_prompt=1, 
            do_classifier_free_guidance=True
        )
        return pe, ne, pp, np # æ­¤æ—¶å…¨æ˜¯ float32

    pe_L, ne_L, pp_L, np_L = get_comp_embeddings(prompt_L)
    pe_R, ne_R, pp_R, np_R = get_comp_embeddings(prompt_R)
    
    # 2. å‡†å¤‡ Time IDs (åˆ†è¾¨ç‡æ¡ä»¶)
    # SDXL éœ€è¦ original_size, crops_coords, target_size
    # pipe._get_add_time_ids è¿”å›çš„æ˜¯ single batchï¼Œæˆ‘ä»¬éœ€è¦æ‹¼æ¥æˆ [Neg, Pos]
    add_time_ids = pipe._get_add_time_ids(
        (height, width), (0, 0), (height, width), 
        dtype=torch.float32, 
        text_encoder_projection_dim=pipe.text_encoder_2.config.projection_dim
    ).to(device)
    add_time_ids = torch.cat([add_time_ids, add_time_ids], dim=0)

    # 3. åˆå§‹åŒ– Latents
    generator = torch.Generator(device=device).manual_seed(seed)
    latents = torch.randn(
        (1, 4, height // 8, width // 8), 
        device=device, 
        generator=generator, 
        dtype=torch.float32
    )
    pipe.scheduler.set_timesteps(steps, device=device)
    latents = latents * pipe.scheduler.init_noise_sigma

    # 4. å‡†å¤‡ Mask
    mask_L, mask_R = get_masks(height, width, device)
    # æ‰©å±•åˆ° [1, 4, H, W]
    mask_L = mask_L.expand(1, 4, -1, -1)
    mask_R = mask_R.expand(1, 4, -1, -1)

    print("ğŸ¨ Denoising Loop (FP32)...")
    
    for i, t in enumerate(pipe.scheduler.timesteps):
        if i % 5 == 0: print(f"Step {i}/{steps}")
        
        # æ‰©å±• latents ç”¨äº CFG (Batch=2: [Latent, Latent])
        latent_input = torch.cat([latents] * 2)
        latent_input = pipe.scheduler.scale_model_input(latent_input, t)
        
        # --- A. å·¦è¾¹å™ªå£° (Left) ---
        noise_pred_L = pipe.unet(
            latent_input,
            t,
            encoder_hidden_states=torch.cat([ne_L, pe_L]),
            added_cond_kwargs={"text_embeds": torch.cat([np_L, pp_L]), "time_ids": add_time_ids},
            return_dict=False
        )[0]
        
        # CFG
        u_L, p_L = noise_pred_L.chunk(2)
        noise_L = u_L + guidance_scale * (p_L - u_L)
        
        # --- B. å³è¾¹å™ªå£° (Right) ---
        noise_pred_R = pipe.unet(
            latent_input,
            t,
            encoder_hidden_states=torch.cat([ne_R, pe_R]),
            added_cond_kwargs={"text_embeds": torch.cat([np_R, pp_R]), "time_ids": add_time_ids},
            return_dict=False
        )[0]
        
        # CFG
        u_R, p_R = noise_pred_R.chunk(2)
        noise_R = u_R + guidance_scale * (p_R - u_R)
        
        # --- C. ç»„åˆ ---
        noise_final = noise_L * mask_L + noise_R * mask_R
        
        # --- D. æ›´æ–° ---
        latents = pipe.scheduler.step(noise_final, t, latents).prev_sample

    # 5. è§£ç 
    print("Decoding...")
    with torch.no_grad():
        # ç¡®ä¿è¾“å…¥æ˜¯ float32 (æœ¬æ¥å°±æ˜¯ï¼Œä½†å†ä¿é™©ä¸€æ¬¡)
        latents = latents.to(torch.float32) / pipe.vae.config.scaling_factor
        image = pipe.vae.decode(latents).sample
        
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).numpy()
    return Image.fromarray((image[0] * 255).astype("uint8"))

# ==============================================================================
# 4. æ‰§è¡Œ
# ==============================================================================
# ç»å…¸æµ‹è¯•ï¼šå·¦çº¢çŒ«ï¼Œå³è“ç‹—
prompt_left = "A red cat, high detail"
prompt_right = "A blue dog, high detail"

final_img = run_rccs(prompt_left, prompt_right, seed=42)
save_path = "fp32_rccs_result.png"
final_img.save(save_path)
print(f"âœ… Success! Image saved to {save_path}")
