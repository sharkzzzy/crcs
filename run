import torch
import torch.nn.functional as F
from diffusers import StableDiffusionXLPipeline
from PIL import Image

# ==============================================================================
# 1. åŸºç¡€è®¾ç½®
# ==============================================================================
device = "cuda"
model_id = "stabilityai/stable-diffusion-xl-base-1.0"

# åŠ è½½æ¨¡å‹
print("ğŸš€ Loading SDXL...")
pipe = StableDiffusionXLPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    variant="fp16",
    use_safetensors=True
).to(device)

# å¯ç”¨ CPU Offload ä»¥èŠ‚çœæ˜¾å­˜ (24G æ˜¾å­˜å…¶å®ä¸å¼€å¯ä¹Ÿèƒ½è·‘ï¼Œå¼€å¯æ›´ç¨³)
# pipe.enable_model_cpu_offload() 

# ==============================================================================
# 2. è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆç®€å•çš„å·¦å³ Mask
# ==============================================================================
def get_masks(height, width, device):
    # ç®€å•çš„å·¦å³åˆ†å± Mask
    # Mask å½¢çŠ¶: [1, 1, H, W] (Latentç©ºé—´)
    latent_h, latent_w = height // 8, width // 8
    
    # 1. å·¦è¾¹ Mask (0.0 - 0.5)
    mask_left = torch.zeros(1, 1, latent_h, latent_w, device=device)
    mask_left[..., :latent_w // 2] = 1.0
    
    # 2. å³è¾¹ Mask (0.5 - 1.0)
    mask_right = torch.zeros(1, 1, latent_h, latent_w, device=device)
    mask_right[..., latent_w // 2:] = 1.0
    
    # 3. èƒŒæ™¯ Mask (å…¨å›¾ï¼Œä½†åœ¨é‡å åŒºä¼šè¢«å‰æ™¯è¦†ç›–)
    # è¿™é‡Œæˆ‘ä»¬ç®€å•åšï¼šå‰æ™¯ä¼˜å…ˆã€‚
    # å®é™…ä¸Šä¸ºäº†èåˆï¼Œå¯ä»¥ç»™ä¸€ç‚¹ç¾½åŒ–ã€‚
    return mask_left, mask_right

# ==============================================================================
# 3. æ ¸å¿ƒï¼šæ‰‹å†™é‡‡æ ·å¾ªç¯ (RCCS Logic)
# ==============================================================================
@torch.no_grad()
def run_rccs(prompt_L, prompt_R, prompt_BG, seed=42):
    height, width = 1024, 1024
    steps = 40
    guidance_scale = 7.5
    
    # 1. å‡†å¤‡ Embeddings
    # å°è£…ä¸€ä¸ª helper æ¥è·å– embedding å’Œ added_cond (time_ids)
    def encode(text):
        (pe, ne, pp, np) = pipe.encode_prompt(
            text, device=device, num_images_per_prompt=1, do_classifier_free_guidance=True
        )
        return pe, ne, pp, np

    # ç¼–ç ä¸‰ä¸ªæç¤ºè¯
    pe_L, ne_L, pp_L, np_L = encode(prompt_L)
    pe_R, ne_R, pp_R, np_R = encode(prompt_R)
    pe_BG, ne_BG, pp_BG, np_BG = encode(prompt_BG)
    
    # å‡†å¤‡ Time IDs (åˆ†è¾¨ç‡ä¿¡æ¯)
    add_time_ids = pipe._get_add_time_ids(
        (height, width), (0, 0), (height, width), dtype=pe_L.dtype, text_encoder_projection_dim=pp_L.shape[-1]
    ).to(device)
    # æ‹¼æˆ CFG éœ€è¦çš„ Batch=2 [Neg, Pos]
    add_time_ids = torch.cat([add_time_ids, add_time_ids], dim=0)

    # 2. åˆå§‹åŒ– Latents
    generator = torch.Generator(device=device).manual_seed(seed)
    latents = torch.randn((1, 4, height//8, width//8), device=device, generator=generator, dtype=pe_L.dtype)
    pipe.scheduler.set_timesteps(steps, device=device)
    latents = latents * pipe.scheduler.init_noise_sigma

    # 3. å‡†å¤‡ Masks
    mask_L, mask_R = get_masks(height, width, device)
    # æ‰©å±•åˆ° latent é€šé“æ•° (4)
    mask_L = mask_L.expand(1, 4, -1, -1).to(dtype=latents.dtype)
    mask_R = mask_R.expand(1, 4, -1, -1).to(dtype=latents.dtype)
    
    # èƒŒæ™¯ Mask: å‰©ä½™éƒ¨åˆ† (è™½ç„¶è¿™é‡Œå·¦å³æ‹¼æ»¡äº†ï¼Œä½†ä¸ºäº†å¹³æ»‘èåˆï¼Œæˆ‘ä»¬å¯ä»¥è®¾ä¸ºå…¨ 1ï¼Œé€šè¿‡åŠ æƒå¹³å‡)
    # ç®€å•ç­–ç•¥ï¼šä¸¥æ ¼åˆ†åŒº
    # Noise = Noise_L * Mask_L + Noise_R * Mask_R
    
    print("ğŸ¨ Starting Denoising Loop...")
    
    for i, t in enumerate(pipe.scheduler.timesteps):
        if i % 10 == 0: print(f"Step {i}/{steps}")
        
        # æ‰©å…… Latents ä»¥é€‚åº” CFG (Batch=2)
        latent_model_input = torch.cat([latents] * 2)
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)
        
        # --- A. è®¡ç®—å·¦è¾¹ä¸»ä½“çš„å™ªå£° (Left Cat) ---
        # ä¼ å…¥ added_cond_kwargs æ˜¯ SDXL çš„å…³é”®
        noise_L = pipe.unet(
            latent_model_input, t, encoder_hidden_states=torch.cat([ne_L, pe_L]),
            added_cond_kwargs={"text_embeds": torch.cat([np_L, pp_L]), "time_ids": add_time_ids}
        ).sample
        # CFG
        n_uncond, n_pos = noise_L.chunk(2)
        noise_L = n_uncond + guidance_scale * (n_pos - n_uncond)
        
        # --- B. è®¡ç®—å³è¾¹ä¸»ä½“çš„å™ªå£° (Right Dog) ---
        noise_R = pipe.unet(
            latent_model_input, t, encoder_hidden_states=torch.cat([ne_R, pe_R]),
            added_cond_kwargs={"text_embeds": torch.cat([np_R, pp_R]), "time_ids": add_time_ids}
        ).sample
        # CFG
        n_uncond, n_pos = noise_R.chunk(2)
        noise_R = n_uncond + guidance_scale * (n_pos - n_uncond)
        
        # --- C. è®¡ç®—èƒŒæ™¯å™ªå£° (å¯é€‰ï¼Œç”¨äºå…œåº•æˆ–å¹³æ»‘) ---
        # å¦‚æœå·¦å³æ‹¼æ»¡äº†ï¼Œè¿™ä¸€æ­¥å¯ä»¥çœã€‚å¦‚æœä¸­é—´æœ‰ç©ºéš™ï¼Œéœ€è¦è¿™ä¸€æ­¥ã€‚
        # è¿™é‡Œæ¼”ç¤ºæœ€ç®€ç‰ˆæœ¬ï¼šç›´æ¥æ‹¼å·¦å³ã€‚
        
        # --- D. ç»„åˆ (Combine) ---
        # æ ¸å¿ƒé€»è¾‘ï¼šåªåœ¨å·¦è¾¹ç”¨å·¦è¾¹çš„å™ªå£°ï¼Œå³è¾¹ç”¨å³è¾¹çš„å™ªå£°
        noise_final = noise_L * mask_L + noise_R * mask_R
        
        # --- E. æ›´æ–° Latents ---
        latents = pipe.scheduler.step(noise_final, t, latents).prev_sample

    # 4. è§£ç 
    print("Decoding...")
    latents = latents / pipe.vae.config.scaling_factor
    image = pipe.vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).float().numpy()
    return Image.fromarray((image[0] * 255).astype("uint8"))

# ==============================================================================
# 4. è¿è¡Œæµ‹è¯•
# ==============================================================================
prompt_left = "A red cat, high quality"
prompt_right = "A blue dog, high quality"
prompt_bg = "A park background" # è¿™é‡Œçš„ç®€å•ç‰ˆæ²¡ç”¨åˆ°èƒŒæ™¯èåˆï¼Œåªåšå·¦å³æ‹¼æ¥

img = run_rccs(prompt_left, prompt_right, prompt_bg)
img.save("simple_rccs_result.png")
print("âœ… Done! Check simple_rccs_result.png")
